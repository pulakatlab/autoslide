{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83d\udd2c AutoSlide: AI-Powered Histological Analysis","text":"<p>Unlock the hidden patterns in your histological slides with deep learning</p> <p>AutoSlide is a comprehensive pipeline that transforms how researchers analyze histological slides, combining computer vision with deep learning to identify tissues, detect vessels, and quantify fibrosis with unprecedented precision.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>Automated Tissue Recognition - Instantly identify and classify different tissue types</li> <li>Smart Region Selection - Automatically extract the most informative regions for analysis</li> <li>Advanced Vessel Detection - Precisely locate and measure blood vessels using Mask R-CNN</li> <li>Fibrosis Quantification - Objectively measure fibrotic changes in tissue samples</li> <li>Reproducible Workflow - Ensure consistent results across multiple samples and studies</li> <li>Comprehensive Data Management - Track annotations and regions with unique hashing system</li> </ul>"},{"location":"#the-autoslide-pipeline","title":"\ud83d\ude80 The AutoSlide Pipeline","text":"<p>Our end-to-end workflow transforms raw histological slides into actionable insights:</p> <ol> <li>\ud83d\udd0d Initial Annotation - Intelligent thresholding and region identification</li> <li>\ud83c\udff7\ufe0f Final Annotation - Precise tissue labeling and mask generation  </li> <li>\ud83d\udcca Region Suggestion - Strategic selection of analysis-ready sections</li> <li>\ud83d\udd2c Vessel Detection - Deep learning-based identification of vascular structures using pre-trained Mask R-CNN</li> <li>\ud83d\udcc8 Fibrosis Quantification - Automated measurement of fibrotic tissue using HSV color analysis</li> </ol>"},{"location":"#why-autoslide","title":"\ud83d\udca1 Why AutoSlide?","text":"<ul> <li>Save Time - Automate tedious manual annotation and region selection</li> <li>Increase Accuracy - Leverage deep learning for consistent, objective analysis</li> <li>Enhance Reproducibility - Standardize your histological analysis workflow</li> <li>Discover More - Identify patterns and features invisible to the human eye</li> <li>Scale Analysis - Process multiple slides efficiently with batch processing</li> </ul>"},{"location":"#technical-highlights","title":"\ud83d\udee0\ufe0f Technical Highlights","text":"<ul> <li>Deep Learning Integration - Pre-trained Mask R-CNN models for precise vessel detection</li> <li>Advanced Image Processing - Sophisticated morphological operations for tissue segmentation</li> <li>Intelligent Selection Algorithms - Context-aware region extraction based on tissue properties</li> <li>Comprehensive Visualization - Rich visual outputs at every stage of the pipeline</li> <li>Data Augmentation - Negative sampling and artificial vessel generation for robust training</li> <li>Unique Section Tracking - SHA-256 based hashing for reproducible section identification</li> </ul>"},{"location":"#data-preparation-model-usage","title":"\ud83d\udcca Data Preparation &amp; Model Usage","text":"<p>AutoSlide includes specialized tools for:</p> <ul> <li>Converting annotations from Labelbox and other labeling tools into analysis-ready formats</li> <li>Creating high-quality binary masks from polygon annotations</li> <li>Utilizing pre-trained state-of-the-art deep learning models for vessel detection</li> <li>Generating insightful visualizations of model predictions</li> <li>Comprehensive data handling and visualization capabilities</li> </ul>"},{"location":"#supported-file-formats","title":"\ud83d\udd2c Supported File Formats","text":"<ul> <li>Input: .svs slide files (Aperio format)</li> <li>Annotations: Labelbox NDJSON exports, CSV metadata</li> <li>Models: PyTorch .pth files</li> <li>Outputs: PNG images, CSV data, JSON tracking files</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>Pipeline Overview</li> <li>API Reference</li> </ul> <p>Ready to transform your histological analysis? Get started with AutoSlide today!</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to AutoSlide! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Clone your fork: <code>git clone https://github.com/your-username/autoslide.git</code></li> <li>Create a branch: <code>git checkout -b feature/your-feature-name</code></li> <li>Make your changes</li> <li>Run tests: <code>pytest tests/</code></li> <li>Commit your changes: <code>git commit -m \"Add feature: description\"</code></li> <li>Push to your fork: <code>git push origin feature/your-feature-name</code></li> <li>Create a Pull Request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>Install development dependencies:</p> <pre><code>pip install -r requirements-dev.txt\n</code></pre> <p>This includes:</p> <ul> <li>pytest for testing</li> <li>black for code formatting</li> <li>isort for import sorting</li> <li>flake8 for linting</li> <li>pre-commit hooks</li> </ul>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We follow PEP 8 style guidelines with some modifications:</p> <ul> <li>Line length: 88 characters (Black default)</li> <li>Use type hints where appropriate</li> <li>Write docstrings for all public functions</li> </ul>"},{"location":"contributing/#formatting","title":"Formatting","text":"<p>Format code with Black:</p> <pre><code>black autoslide/\n</code></pre> <p>Sort imports with isort:</p> <pre><code>isort autoslide/\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p>Run flake8:</p> <pre><code>flake8 autoslide/\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Write tests for new features:</p> <pre><code># Run all tests\npytest tests/\n\n# Run specific test file\npytest tests/test_pipeline.py\n\n# Run with coverage\npytest --cov=autoslide tests/\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks:</p> <pre><code>pre-commit install\n</code></pre> <p>This automatically runs formatting and linting before each commit.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Update documentation when adding features:</p> <ol> <li>Add docstrings to new functions</li> <li>Update relevant markdown files in <code>docs/</code></li> <li>Add examples if applicable</li> <li>Update API reference if needed</li> </ol> <p>Build documentation locally:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Clear description - Explain what changes you made and why</li> <li>Reference issues - Link to related issues</li> <li>Tests - Include tests for new features</li> <li>Documentation - Update docs as needed</li> <li>Small PRs - Keep changes focused and manageable</li> <li>Clean history - Squash commits if necessary</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>When reporting bugs, include:</p> <ul> <li>AutoSlide version</li> <li>Python version</li> <li>Operating system</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Error messages and stack traces</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For feature requests, describe:</p> <ul> <li>The problem you're trying to solve</li> <li>Your proposed solution</li> <li>Alternative approaches considered</li> <li>Potential impact on existing functionality</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Focus on constructive feedback</li> <li>Assume good intentions</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Open an issue for questions</li> <li>Check existing issues and documentation first</li> <li>Provide context and examples</li> </ul> <p>Thank you for contributing to AutoSlide!</p>"},{"location":"license/","title":"License","text":"<p>AutoSlide is released under the MIT License.</p>"},{"location":"license/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2024 Abuzar Mahmood</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"license/#third-party-licenses","title":"Third-Party Licenses","text":"<p>AutoSlide uses several open-source libraries, each with their own licenses:</p> <ul> <li>PyTorch - BSD-style license</li> <li>torchvision - BSD license</li> <li>OpenCV - Apache 2.0 license</li> <li>scikit-learn - BSD license</li> <li>NumPy - BSD license</li> <li>Pillow - HPND license</li> <li>matplotlib - PSF-based license</li> <li>pandas - BSD license</li> </ul> <p>Please refer to each library's documentation for full license details.</p>"},{"location":"advanced/arbitrary-directory-prediction/","title":"Arbitrary Directory Prediction","text":""},{"location":"advanced/arbitrary-directory-prediction/#overview","title":"Overview","text":"<p>This feature allows the neural network to perform predictions on any directory containing images, not just the configured <code>suggested_regions</code> directory. This addresses issue #82 and provides flexibility for processing images from various sources.</p>"},{"location":"advanced/arbitrary-directory-prediction/#features","title":"Features","text":"<ul> <li>Flexible input: Process any directory containing images</li> <li>Custom output location: Specify where predictions should be saved</li> <li>Multiple image formats: Supports PNG, JPG, JPEG, TIF, TIFF</li> <li>Visualization support: Optional overlay generation for quality control</li> <li>Progress tracking: Real-time progress updates during processing</li> </ul>"},{"location":"advanced/arbitrary-directory-prediction/#usage","title":"Usage","text":""},{"location":"advanced/arbitrary-directory-prediction/#basic-usage","title":"Basic Usage","text":"<p>Process all images in a directory:</p> <pre><code>python -m autoslide.pipeline.model.prediction --dir /path/to/images\n</code></pre> <p>This will: - Load images from <code>/path/to/images</code> - Save predictions to <code>/path/to/images/predictions/masks/</code> - Display progress and summary</p>"},{"location":"advanced/arbitrary-directory-prediction/#custom-output-directory","title":"Custom Output Directory","text":"<p>Specify a custom output location:</p> <pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /path/to/images \\\n    --output-dir /path/to/results\n</code></pre> <p>Predictions will be saved to: - Masks: <code>/path/to/results/masks/</code> - Overlays (if enabled): <code>/path/to/results/overlays/</code></p>"},{"location":"advanced/arbitrary-directory-prediction/#with-visualizations","title":"With Visualizations","text":"<p>Generate overlay visualizations for quality control:</p> <pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /path/to/images \\\n    --save-visualizations\n</code></pre> <p>This creates overlay images showing predictions on top of original images.</p>"},{"location":"advanced/arbitrary-directory-prediction/#custom-model","title":"Custom Model","text":"<p>Use a specific model file:</p> <pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /path/to/images \\\n    --model-path /path/to/custom_model.pth\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#limit-processing","title":"Limit Processing","text":"<p>Process only a subset of images (useful for testing):</p> <pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /path/to/images \\\n    --max-images 10\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#verbose-output","title":"Verbose Output","text":"<p>Get detailed information during processing:</p> <pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /path/to/images \\\n    --verbose\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#complete-example","title":"Complete Example","text":"<pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /data/external_dataset/images \\\n    --output-dir /data/external_dataset/predictions \\\n    --model-path artifacts/best_model.pth \\\n    --save-visualizations \\\n    --max-images 100 \\\n    --verbose\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#output-structure","title":"Output Structure","text":"<p>When using <code>--dir</code>, the output structure is:</p> <pre><code>&lt;output-dir&gt;/\n\u251c\u2500\u2500 masks/\n\u2502   \u251c\u2500\u2500 image1_mask.png\n\u2502   \u251c\u2500\u2500 image2_mask.png\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 overlays/  (if --save-visualizations is used)\n    \u251c\u2500\u2500 image1_overlay.png\n    \u251c\u2500\u2500 image2_overlay.png\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#supported-image-formats","title":"Supported Image Formats","text":"<p>The following image formats are automatically detected: - PNG (<code>.png</code>, <code>.PNG</code>) - JPEG (<code>.jpg</code>, <code>.JPG</code>, <code>.jpeg</code>, <code>.JPEG</code>) - TIFF (<code>.tif</code>, <code>.TIF</code>, <code>.tiff</code>, <code>.TIFF</code>)</p>"},{"location":"advanced/arbitrary-directory-prediction/#use-cases","title":"Use Cases","text":""},{"location":"advanced/arbitrary-directory-prediction/#1-external-dataset-processing","title":"1. External Dataset Processing","text":"<p>Process images from a collaborator or external source:</p> <pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /data/collaborator_images \\\n    --output-dir /data/collaborator_results\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#2-quality-control","title":"2. Quality Control","text":"<p>Generate visualizations for a sample of images:</p> <pre><code>python -m autoslide.pipeline.model.prediction \\\n    --dir /data/validation_set \\\n    --save-visualizations \\\n    --max-images 50\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#3-batch-processing","title":"3. Batch Processing","text":"<p>Process multiple directories in a script:</p> <pre><code>#!/bin/bash\nfor dir in /data/batch_*; do\n    python -m autoslide.pipeline.model.prediction \\\n        --dir \"$dir/images\" \\\n        --output-dir \"$dir/predictions\"\ndone\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#4-model-comparison","title":"4. Model Comparison","text":"<p>Compare different models on the same dataset:</p> <pre><code># Model 1\npython -m autoslide.pipeline.model.prediction \\\n    --dir /data/test_set \\\n    --output-dir /results/model1 \\\n    --model-path models/model1.pth\n\n# Model 2\npython -m autoslide.pipeline.model.prediction \\\n    --dir /data/test_set \\\n    --output-dir /results/model2 \\\n    --model-path models/model2.pth\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#integration-with-color-normalization","title":"Integration with Color Normalization","text":"<p>Combine with histogram percentile normalization for best results:</p> <pre><code># Step 1: Normalize colors\npython -c \"\nfrom autoslide.pipeline.model.color_normalization import batch_histogram_normalization\nbatch_histogram_normalization(\n    input_dir='/data/new_dataset/images',\n    output_dir='/data/new_dataset/normalized',\n    reference_images=['training_data/ref1.png', 'training_data/ref2.png']\n)\n\"\n\n# Step 2: Run predictions on normalized images\npython -m autoslide.pipeline.model.prediction \\\n    --dir /data/new_dataset/normalized \\\n    --output-dir /data/new_dataset/predictions\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#python-api","title":"Python API","text":"<p>You can also use the functionality programmatically:</p> <pre><code>from autoslide.pipeline.model.prediction import process_arbitrary_directory\n\nprocess_arbitrary_directory(\n    input_dir='/path/to/images',\n    output_dir='/path/to/results',\n    model_path='artifacts/best_model.pth',\n    save_visualizations=True,\n    max_images=None,  # Process all images\n    verbose=True\n)\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#comparison-with-default-mode","title":"Comparison with Default Mode","text":"Feature Default Mode Arbitrary Directory Mode Input source Config <code>suggested_regions</code> Any directory via <code>--dir</code> Output location Alongside input images Customizable via <code>--output-dir</code> Directory structure Preserves SVS structure Flat structure Reprocessing <code>--reprocess</code> flag Always processes all images Use case Standard pipeline External datasets, testing"},{"location":"advanced/arbitrary-directory-prediction/#command-line-arguments","title":"Command Line Arguments","text":""},{"location":"advanced/arbitrary-directory-prediction/#new-arguments-issue-82","title":"New Arguments (Issue #82)","text":"<ul> <li><code>--dir &lt;path&gt;</code>: Directory containing images to process</li> <li><code>--output-dir &lt;path&gt;</code>: Output directory for predictions (default: <code>&lt;dir&gt;/predictions</code>)</li> </ul>"},{"location":"advanced/arbitrary-directory-prediction/#existing-arguments-still-available","title":"Existing Arguments (Still Available)","text":"<ul> <li><code>--model-path &lt;path&gt;</code>: Path to saved model</li> <li><code>--save-visualizations</code>: Save prediction visualizations</li> <li><code>--max-images &lt;n&gt;</code>: Maximum number of images to process</li> <li><code>--verbose</code>, <code>-v</code>: Print detailed information</li> <li><code>--reprocess</code>: (Only for default mode) Remove existing outputs and reprocess</li> </ul>"},{"location":"advanced/arbitrary-directory-prediction/#error-handling","title":"Error Handling","text":"<p>The script handles various error conditions gracefully:</p> <ul> <li>No images found: Displays message and exits</li> <li>Model loading failure: Shows error and exits</li> <li>Individual image failures: Logs error, continues with remaining images</li> <li>Invalid paths: Validates paths and shows helpful error messages</li> </ul>"},{"location":"advanced/arbitrary-directory-prediction/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Memory usage: Processes images one at a time to minimize memory footprint</li> <li>GPU acceleration: Automatically uses GPU if available</li> <li>Batch size: Single image processing for arbitrary directories (no batching)</li> <li>Progress tracking: Uses tqdm for real-time progress updates</li> </ul>"},{"location":"advanced/arbitrary-directory-prediction/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ol> <li>Test first: Use <code>--max-images 10</code> to test on a small subset</li> <li>Check visualizations: Use <code>--save-visualizations</code> for quality control</li> <li>Normalize colors: Apply color normalization before prediction for best results</li> <li>Organize outputs: Use descriptive output directory names</li> <li>Monitor progress: Use <code>--verbose</code> for detailed logging</li> </ol>"},{"location":"advanced/arbitrary-directory-prediction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/arbitrary-directory-prediction/#issue-no-images-found","title":"Issue: No images found","text":"<p>Cause: Directory doesn't contain supported image formats</p> <p>Solution: Check that images have supported extensions (.png, .jpg, .tif, etc.)</p>"},{"location":"advanced/arbitrary-directory-prediction/#issue-out-of-memory","title":"Issue: Out of memory","text":"<p>Cause: Images are too large or GPU memory is limited</p> <p>Solution:  - Process fewer images at a time using <code>--max-images</code> - Use CPU instead of GPU (set <code>CUDA_VISIBLE_DEVICES=\"\"</code>)</p>"},{"location":"advanced/arbitrary-directory-prediction/#issue-predictions-look-incorrect","title":"Issue: Predictions look incorrect","text":"<p>Cause: Color distribution differs from training data</p> <p>Solution: Apply color normalization before prediction: <pre><code># Normalize first\npython -c \"from autoslide.pipeline.model.color_normalization import batch_histogram_normalization; batch_histogram_normalization('input', 'normalized', 'ref.png')\"\n\n# Then predict\npython -m autoslide.pipeline.model.prediction --dir normalized\n</code></pre></p>"},{"location":"advanced/arbitrary-directory-prediction/#issue-slow-processing","title":"Issue: Slow processing","text":"<p>Cause: CPU processing or large images</p> <p>Solution: - Ensure GPU is available and being used - Check image sizes (very large images take longer) - Use <code>--verbose</code> to see per-image timing</p>"},{"location":"advanced/arbitrary-directory-prediction/#examples","title":"Examples","text":""},{"location":"advanced/arbitrary-directory-prediction/#example-1-quick-test","title":"Example 1: Quick Test","text":"<pre><code># Test on 5 images with visualizations\npython -m autoslide.pipeline.model.prediction \\\n    --dir test_images \\\n    --max-images 5 \\\n    --save-visualizations \\\n    --verbose\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#example-2-production-processing","title":"Example 2: Production Processing","text":"<pre><code># Process entire dataset with custom model\npython -m autoslide.pipeline.model.prediction \\\n    --dir /data/production/images \\\n    --output-dir /data/production/predictions \\\n    --model-path /models/production_v2.pth\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#example-3-batch-script","title":"Example 3: Batch Script","text":"<pre><code>#!/bin/bash\n# Process multiple datasets\n\nDATASETS=(\n    \"/data/dataset1\"\n    \"/data/dataset2\"\n    \"/data/dataset3\"\n)\n\nMODEL=\"/models/best_model.pth\"\n\nfor dataset in \"${DATASETS[@]}\"; do\n    echo \"Processing $dataset...\"\n    python -m autoslide.pipeline.model.prediction \\\n        --dir \"$dataset/images\" \\\n        --output-dir \"$dataset/predictions\" \\\n        --model-path \"$MODEL\" \\\n        --save-visualizations\ndone\n\necho \"All datasets processed!\"\n</code></pre>"},{"location":"advanced/arbitrary-directory-prediction/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements: - Recursive directory processing - Parallel processing for multiple images - Support for additional image formats - Automatic color normalization option - Batch prediction for efficiency - Progress saving and resumption</p>"},{"location":"advanced/arbitrary-directory-prediction/#related-documentation","title":"Related Documentation","text":"<ul> <li>Histogram Percentile Normalization</li> <li>Color Correction</li> <li>Vessel Detection</li> </ul>"},{"location":"advanced/arbitrary-directory-prediction/#contributing","title":"Contributing","text":"<p>When extending this feature: 1. Maintain backward compatibility with default mode 2. Add tests for new functionality 3. Update this documentation 4. Follow existing code patterns</p>"},{"location":"advanced/arbitrary-directory-prediction/#license","title":"License","text":"<p>This feature is part of the autoslide project and follows the same MIT license.</p>"},{"location":"advanced/color-correction/","title":"Color Correction Preprocessing","text":""},{"location":"advanced/color-correction/#overview","title":"Overview","text":"<p>This module implements color correction preprocessing for histological images to normalize color variations across different datasets or scanning sessions. This addresses issue #80 by providing project-level color correction using reference images from the original dataset.</p>"},{"location":"advanced/color-correction/#features","title":"Features","text":"<ul> <li>Multiple Color Transfer Methods: Supports Reinhard color transfer and histogram matching</li> <li>Reference-Based Correction: Uses example images from the original dataset to correct new datasets</li> <li>Batch Processing: Efficiently process entire directories of images</li> <li>Color Divergence Detection: Test for color distribution differences to determine if correction is needed</li> <li>Integration Ready: Seamlessly integrates with existing preprocessing pipeline</li> </ul>"},{"location":"advanced/color-correction/#installation","title":"Installation","text":"<p>No additional dependencies required! The module uses existing dependencies: - <code>numpy &gt;= 1.20.0</code> - <code>opencv-python &gt;= 4.5.0</code> - <code>scipy &gt;= 1.7.0</code></p>"},{"location":"advanced/color-correction/#usage","title":"Usage","text":""},{"location":"advanced/color-correction/#basic-usage","title":"Basic Usage","text":"<pre><code>from autoslide.pipeline.model.color_correction import ColorCorrector\nimport cv2\n\n# Initialize with reference image(s) from original dataset\ncorrector = ColorCorrector('path/to/reference_image.png')\n\n# Load and correct an image\nimg = cv2.imread('new_dataset/image.png')\ncorrected = corrector.correct_image(img, method='reinhard')\ncv2.imwrite('corrected_image.png', corrected)\n</code></pre>"},{"location":"advanced/color-correction/#using-multiple-reference-images","title":"Using Multiple Reference Images","text":"<pre><code># Use multiple reference images for more robust statistics\ncorrector = ColorCorrector([\n    'original_dataset/sample1.png',\n    'original_dataset/sample2.png',\n    'original_dataset/sample3.png'\n])\n\ncorrected = corrector.correct_image(img, method='reinhard')\n</code></pre>"},{"location":"advanced/color-correction/#batch-processing","title":"Batch Processing","text":"<pre><code>from autoslide.pipeline.model.data_preprocessing import apply_color_correction\n\n# Correct all images in a directory\nsuccessful, failed = apply_color_correction(\n    img_dir='data/new_dataset/images',\n    output_dir='data/new_dataset/corrected',\n    reference_images=[\n        'data/original_dataset/ref1.png',\n        'data/original_dataset/ref2.png'\n    ],\n    method='reinhard',\n    file_pattern='*.png'\n)\n\nprint(f\"Corrected {successful} images, {failed} failed\")\n</code></pre>"},{"location":"advanced/color-correction/#detecting-color-divergence","title":"Detecting Color Divergence","text":"<pre><code>from autoslide.pipeline.model.color_correction import compute_color_divergence\nimport cv2\n\n# Load images from different datasets\nref_img = cv2.imread('original_dataset/reference.png')\nnew_img = cv2.imread('new_dataset/sample.png')\n\n# Compute divergence\ndivergence = compute_color_divergence(ref_img, new_img, metric='euclidean')\nprint(f'Color divergence: {divergence}')\n\n# Define threshold based on your requirements\nDIVERGENCE_THRESHOLD = 50.0\n\nif divergence &gt; DIVERGENCE_THRESHOLD:\n    print('\u26a0\ufe0f  High color divergence detected - color correction recommended')\nelse:\n    print('\u2705 Color distributions are similar - no correction needed')\n</code></pre>"},{"location":"advanced/color-correction/#color-correction-methods","title":"Color Correction Methods","text":""},{"location":"advanced/color-correction/#reinhard-color-transfer-recommended","title":"Reinhard Color Transfer (Recommended)","text":"<p>Based on \"Color Transfer between Images\" by Reinhard et al., 2001. This method: - Converts images to LAB color space - Matches mean and standard deviation of each channel - Preserves image structure while adjusting colors - Works well for histological images</p> <pre><code>corrected = corrector.correct_image(img, method='reinhard')\n</code></pre>"},{"location":"advanced/color-correction/#histogram-matching","title":"Histogram Matching","text":"<p>Matches the histogram distribution of the target image to the reference: - Operates in LAB color space - Adjusts each channel independently - Good for images with similar content</p> <pre><code>corrected = corrector.correct_image(img, method='histogram')\n</code></pre>"},{"location":"advanced/color-correction/#divergence-metrics","title":"Divergence Metrics","text":""},{"location":"advanced/color-correction/#euclidean-distance-default","title":"Euclidean Distance (Default)","text":"<p>Computes Euclidean distance between color statistics: <pre><code>divergence = compute_color_divergence(img1, img2, metric='euclidean')\n</code></pre></p>"},{"location":"advanced/color-correction/#manhattan-distance","title":"Manhattan Distance","text":"<p>Computes Manhattan (L1) distance: <pre><code>divergence = compute_color_divergence(img1, img2, metric='manhattan')\n</code></pre></p>"},{"location":"advanced/color-correction/#kl-divergence","title":"KL Divergence","text":"<p>Approximates KL divergence assuming Gaussian distributions: <pre><code>divergence = compute_color_divergence(img1, img2, metric='kl_divergence')\n</code></pre></p>"},{"location":"advanced/color-correction/#integration-with-training-pipeline","title":"Integration with Training Pipeline","text":""},{"location":"advanced/color-correction/#before-training","title":"Before Training","text":"<pre><code>from autoslide.pipeline.model.data_preprocessing import (\n    apply_color_correction,\n    load_data,\n    split_train_val\n)\n\n# Step 1: Apply color correction to new dataset\nprint(\"Applying color correction...\")\napply_color_correction(\n    img_dir='data/new_dataset/images',\n    output_dir='data/new_dataset/corrected',\n    reference_images=[\n        'data/original_dataset/sample1.png',\n        'data/original_dataset/sample2.png'\n    ],\n    method='reinhard'\n)\n\n# Step 2: Load corrected data for training\nlabelled_data_dir, img_dir, mask_dir, image_names, mask_names = load_data()\n\n# Step 3: Continue with normal training pipeline\ntrain_imgs, train_masks, val_imgs, val_masks = split_train_val(\n    image_names, mask_names\n)\n# ... rest of training code ...\n</code></pre>"},{"location":"advanced/color-correction/#quality-control-check","title":"Quality Control Check","text":"<pre><code>from autoslide.pipeline.model.color_correction import compute_color_divergence\nimport cv2\nimport os\n\ndef check_dataset_color_consistency(dataset_dir, reference_img_path, threshold=50.0):\n    \"\"\"Check if a dataset needs color correction.\"\"\"\n    ref_img = cv2.imread(reference_img_path)\n\n    divergences = []\n    for img_name in os.listdir(dataset_dir):\n        img_path = os.path.join(dataset_dir, img_name)\n        img = cv2.imread(img_path)\n        if img is not None:\n            div = compute_color_divergence(ref_img, img, metric='euclidean')\n            divergences.append(div)\n\n    avg_divergence = sum(divergences) / len(divergences)\n    max_divergence = max(divergences)\n\n    print(f\"Average divergence: {avg_divergence:.2f}\")\n    print(f\"Maximum divergence: {max_divergence:.2f}\")\n\n    if avg_divergence &gt; threshold:\n        print(\"\u26a0\ufe0f  Color correction recommended\")\n        return True\n    else:\n        print(\"\u2705 Dataset color is consistent\")\n        return False\n\n# Usage\nneeds_correction = check_dataset_color_consistency(\n    'data/new_dataset/images',\n    'data/original_dataset/reference.png'\n)\n</code></pre>"},{"location":"advanced/color-correction/#testing","title":"Testing","text":"<p>The module includes comprehensive tests in <code>tests/test_color_correction.py</code>:</p> <pre><code># Run tests (requires pytest and dependencies installed)\npytest tests/test_color_correction.py -v\n</code></pre> <p>Test coverage includes: - Color corrector initialization - Reinhard color transfer - Histogram matching - Color divergence computation - Batch processing - Integration scenarios</p>"},{"location":"advanced/color-correction/#api-reference","title":"API Reference","text":""},{"location":"advanced/color-correction/#colorcorrector-class","title":"ColorCorrector Class","text":"<pre><code>class ColorCorrector:\n    def __init__(self, reference_images: Union[str, Path, List[Union[str, Path]]])\n    def correct_image(self, image: np.ndarray, method: str = 'reinhard') -&gt; np.ndarray\n    def correct_image_file(self, input_path: Union[str, Path], \n                          output_path: Union[str, Path], \n                          method: str = 'reinhard') -&gt; bool\n    def get_color_statistics(self, image: np.ndarray) -&gt; dict\n</code></pre>"},{"location":"advanced/color-correction/#functions","title":"Functions","text":"<pre><code>def compute_color_divergence(\n    image1: np.ndarray,\n    image2: np.ndarray,\n    metric: str = 'euclidean'\n) -&gt; float\n\ndef batch_color_correction(\n    input_dir: Union[str, Path],\n    output_dir: Union[str, Path],\n    reference_images: Union[str, Path, List[Union[str, Path]]],\n    method: str = 'reinhard',\n    file_pattern: str = '*.png'\n) -&gt; Tuple[int, int]\n\ndef apply_color_correction(\n    img_dir: str,\n    output_dir: str,\n    reference_images: Union[str, List[str]],\n    method: str = 'reinhard',\n    file_pattern: str = '*.png'\n) -&gt; Tuple[int, int]\n</code></pre>"},{"location":"advanced/color-correction/#technical-details","title":"Technical Details","text":""},{"location":"advanced/color-correction/#color-space","title":"Color Space","text":"<p>All color correction operations are performed in LAB color space because: - LAB is perceptually uniform - Separates luminance (L) from color (A, B) - Better for color manipulation than RGB - Standard in color science applications</p>"},{"location":"advanced/color-correction/#algorithm-reinhard-color-transfer","title":"Algorithm: Reinhard Color Transfer","text":"<ol> <li>Convert images to LAB color space</li> <li>Compute mean (\u03bc) and standard deviation (\u03c3) for each channel</li> <li>For target image:</li> <li>Subtract target mean: <code>I' = I - \u03bc_target</code></li> <li>Scale by std ratio: <code>I'' = I' \u00d7 (\u03c3_ref / \u03c3_target)</code></li> <li>Add reference mean: <code>I''' = I'' + \u03bc_ref</code></li> <li>Convert back to BGR color space</li> </ol>"},{"location":"advanced/color-correction/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Processing time: ~10-50ms per 512\u00d7512 image (CPU)</li> <li>Memory usage: ~3\u00d7 image size (for color space conversions)</li> <li>Batch processing recommended for large datasets</li> <li>Can be parallelized for multiple images</li> </ul>"},{"location":"advanced/color-correction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/color-correction/#issue-colors-look-oversaturated","title":"Issue: Colors look oversaturated","text":"<p>Try using histogram matching instead: <pre><code>corrected = corrector.correct_image(img, method='histogram')\n</code></pre></p>"},{"location":"advanced/color-correction/#issue-reference-images-have-different-characteristics","title":"Issue: Reference images have different characteristics","text":"<p>Use multiple reference images to get more robust statistics: <pre><code>corrector = ColorCorrector([\n    'ref1.png', 'ref2.png', 'ref3.png', 'ref4.png'\n])\n</code></pre></p>"},{"location":"advanced/color-correction/#issue-correction-too-aggressive","title":"Issue: Correction too aggressive","text":"<p>The Reinhard method preserves relative color relationships. If results are too different: 1. Check that reference images are representative 2. Verify input images are in correct format (BGR) 3. Consider using images from the same staining batch as references</p>"},{"location":"advanced/color-correction/#references","title":"References","text":"<ol> <li> <p>Reinhard, E., Adhikhmin, M., Gooch, B., &amp; Shirley, P. (2001). Color transfer between images. IEEE Computer Graphics and Applications, 21(5), 34-41.</p> </li> <li> <p>ColorTransferLib: https://github.com/hpotechius/ColorTransferLib</p> </li> <li>Comprehensive library for color transfer algorithms</li> <li>Can be integrated for additional methods if needed</li> </ol>"},{"location":"advanced/color-correction/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements for future versions: - Integration with ColorTransferLib for additional methods - GPU acceleration for batch processing - Automatic reference image selection - Adaptive threshold determination for divergence - Support for other color spaces (HSV, YCbCr) - Stain normalization specific to H&amp;E images</p>"},{"location":"advanced/color-correction/#contributing","title":"Contributing","text":"<p>When adding new color correction methods: 1. Add method to <code>ColorCorrector.correct_image()</code> 2. Add corresponding tests in <code>tests/test_color_correction.py</code> 3. Update this documentation 4. Ensure backward compatibility</p>"},{"location":"advanced/color-correction/#license","title":"License","text":"<p>This module is part of the autoslide project and follows the same MIT license.</p>"},{"location":"advanced/histogram-normalization/","title":"Histogram Percentile Color Normalization","text":""},{"location":"advanced/histogram-normalization/#overview","title":"Overview","text":"<p>This module implements histogram percentile-based color normalization for histological images. This method is particularly effective for normalizing staining intensity variations across different batches or scanning sessions.</p>"},{"location":"advanced/histogram-normalization/#features","title":"Features","text":"<ul> <li>Percentile-based normalization: Maps histogram percentiles between reference and target images</li> <li>Robust to outliers: Uses percentile clipping to handle extreme values</li> <li>Batch processing: Efficiently process entire directories</li> <li>Histogram comparison: Utilities to compare color distributions</li> <li>Customizable percentiles: Adjust percentile range for different use cases</li> </ul>"},{"location":"advanced/histogram-normalization/#installation","title":"Installation","text":"<p>No additional dependencies required! Uses existing dependencies: - <code>numpy &gt;= 1.20.0</code> - <code>opencv-python &gt;= 4.5.0</code></p>"},{"location":"advanced/histogram-normalization/#usage","title":"Usage","text":""},{"location":"advanced/histogram-normalization/#basic-usage","title":"Basic Usage","text":"<pre><code>from autoslide.pipeline.model.color_normalization import HistogramPercentileNormalizer\nimport cv2\n\n# Initialize with reference image(s)\nnormalizer = HistogramPercentileNormalizer('path/to/reference_image.png')\n\n# Load and normalize an image\nimg = cv2.imread('new_dataset/image.png')\nnormalized = normalizer.normalize_image(img)\ncv2.imwrite('normalized_image.png', normalized)\n</code></pre>"},{"location":"advanced/histogram-normalization/#custom-percentile-range","title":"Custom Percentile Range","text":"<pre><code># Use different percentile range (default is 1.0, 99.0)\nnormalizer = HistogramPercentileNormalizer(\n    'reference.png',\n    percentiles=(5.0, 95.0)  # More aggressive clipping\n)\n\nnormalized = normalizer.normalize_image(img)\n</code></pre>"},{"location":"advanced/histogram-normalization/#multiple-reference-images","title":"Multiple Reference Images","text":"<pre><code># Use multiple reference images for more robust statistics\nnormalizer = HistogramPercentileNormalizer([\n    'original_dataset/sample1.png',\n    'original_dataset/sample2.png',\n    'original_dataset/sample3.png'\n])\n\nnormalized = normalizer.normalize_image(img)\n</code></pre>"},{"location":"advanced/histogram-normalization/#batch-processing","title":"Batch Processing","text":"<pre><code>from autoslide.pipeline.model.color_normalization import batch_histogram_normalization\n\n# Normalize all images in a directory\nsuccessful, failed = batch_histogram_normalization(\n    input_dir='data/new_dataset/images',\n    output_dir='data/new_dataset/normalized',\n    reference_images=['data/original/ref1.png', 'data/original/ref2.png'],\n    percentiles=(1.0, 99.0),\n    file_pattern='*.png'\n)\n\nprint(f\"Normalized {successful} images, {failed} failed\")\n</code></pre>"},{"location":"advanced/histogram-normalization/#histogram-comparison","title":"Histogram Comparison","text":"<pre><code>from autoslide.pipeline.model.color_normalization import compare_histograms\nimport cv2\n\n# Compare histograms between two images\nimg1 = cv2.imread('dataset1/image.png')\nimg2 = cv2.imread('dataset2/image.png')\n\nmetrics = compare_histograms(img1, img2)\n\nfor channel in ['channel_0', 'channel_1', 'channel_2']:\n    print(f\"{channel}:\")\n    print(f\"  Correlation: {metrics[channel]['correlation']:.4f}\")\n    print(f\"  Chi-square: {metrics[channel]['chi_square']:.4f}\")\n    print(f\"  Intersection: {metrics[channel]['intersection']:.4f}\")\n</code></pre>"},{"location":"advanced/histogram-normalization/#how-it-works","title":"How It Works","text":""},{"location":"advanced/histogram-normalization/#algorithm","title":"Algorithm","text":"<p>The histogram percentile normalization method works as follows:</p> <ol> <li> <p>Compute reference percentiles: For each color channel in the reference image(s), compute the values at the specified percentiles (e.g., 1st and 99th percentiles).</p> </li> <li> <p>Compute target percentiles: For each color channel in the target image, compute the same percentiles.</p> </li> <li> <p>Linear mapping: Map the target percentile range to the reference percentile range using linear transformation:    <pre><code>out = (in - low_target) / (high_target - low_target) * (high_ref - low_ref) + low_ref\n</code></pre></p> </li> <li> <p>Clipping: Clip values to the valid range [0, 255].</p> </li> </ol>"},{"location":"advanced/histogram-normalization/#why-percentiles","title":"Why Percentiles?","text":"<ul> <li>Robust to outliers: Unlike using min/max values, percentiles are not affected by extreme outliers</li> <li>Preserves structure: The relative relationships between pixel intensities are maintained</li> <li>Handles staining variations: Effectively normalizes differences in staining intensity</li> <li>Customizable: Can adjust percentile range based on data characteristics</li> </ul>"},{"location":"advanced/histogram-normalization/#comparison-with-other-methods","title":"Comparison with Other Methods","text":"Method Pros Cons Best For Histogram Percentile Robust to outliers, simple, fast May not preserve exact color relationships Staining intensity variations Reinhard Color Transfer Preserves color relationships, works in LAB space More complex, slower General color correction Histogram Matching Exact histogram matching Sensitive to outliers Similar image content"},{"location":"advanced/histogram-normalization/#use-cases","title":"Use Cases","text":""},{"location":"advanced/histogram-normalization/#1-normalizing-staining-intensity","title":"1. Normalizing Staining Intensity","text":"<p>When different tissue batches are stained with varying intensity:</p> <pre><code>normalizer = HistogramPercentileNormalizer(\n    'well_stained_reference.png',\n    percentiles=(1.0, 99.0)\n)\n\n# Normalize under-stained images\nnormalized = normalizer.normalize_image(under_stained_img)\n</code></pre>"},{"location":"advanced/histogram-normalization/#2-scanner-calibration","title":"2. Scanner Calibration","text":"<p>When images from different scanners have different intensity ranges:</p> <pre><code># Use images from Scanner A as reference\nnormalizer = HistogramPercentileNormalizer([\n    'scanner_a/image1.png',\n    'scanner_a/image2.png'\n])\n\n# Normalize images from Scanner B\nnormalized = normalizer.normalize_image(scanner_b_img)\n</code></pre>"},{"location":"advanced/histogram-normalization/#3-quality-control","title":"3. Quality Control","text":"<p>Check if normalization is needed:</p> <pre><code>from autoslide.pipeline.model.color_normalization import compare_histograms\n\nref_img = cv2.imread('reference.png')\nnew_img = cv2.imread('new_image.png')\n\nmetrics = compare_histograms(ref_img, new_img)\n\n# Check correlation for each channel\nfor channel in ['channel_0', 'channel_1', 'channel_2']:\n    correlation = metrics[channel]['correlation']\n    if correlation &lt; 0.8:  # Threshold can be adjusted\n        print(f\"\u26a0\ufe0f  {channel}: Low correlation ({correlation:.3f}) - normalization recommended\")\n    else:\n        print(f\"\u2705 {channel}: Good correlation ({correlation:.3f})\")\n</code></pre>"},{"location":"advanced/histogram-normalization/#integration-with-training-pipeline","title":"Integration with Training Pipeline","text":""},{"location":"advanced/histogram-normalization/#before-training","title":"Before Training","text":"<pre><code>from autoslide.pipeline.model.color_normalization import batch_histogram_normalization\n\n# Normalize new dataset before training\nprint(\"Applying histogram percentile normalization...\")\nbatch_histogram_normalization(\n    input_dir='data/new_dataset/images',\n    output_dir='data/new_dataset/normalized',\n    reference_images=[\n        'data/original_dataset/sample1.png',\n        'data/original_dataset/sample2.png'\n    ],\n    percentiles=(1.0, 99.0)\n)\n\n# Then proceed with training using normalized images\n# ... rest of training code ...\n</code></pre>"},{"location":"advanced/histogram-normalization/#during-prediction","title":"During Prediction","text":"<pre><code>from autoslide.pipeline.model.color_normalization import HistogramPercentileNormalizer\n\n# Initialize normalizer with training dataset references\nnormalizer = HistogramPercentileNormalizer(\n    ['training_data/ref1.png', 'training_data/ref2.png']\n)\n\n# Normalize images before prediction\nimg = cv2.imread('new_image.png')\nnormalized = normalizer.normalize_image(img)\n\n# Perform prediction on normalized image\n# ... prediction code ...\n</code></pre>"},{"location":"advanced/histogram-normalization/#api-reference","title":"API Reference","text":""},{"location":"advanced/histogram-normalization/#histogrampercentilenormalizer-class","title":"HistogramPercentileNormalizer Class","text":"<pre><code>class HistogramPercentileNormalizer:\n    def __init__(\n        self,\n        reference_images: Union[str, Path, List[Union[str, Path]]],\n        percentiles: Tuple[float, float] = (1.0, 99.0)\n    )\n\n    def normalize_image(self, image: np.ndarray) -&gt; np.ndarray\n\n    def normalize_image_file(\n        self,\n        input_path: Union[str, Path],\n        output_path: Union[str, Path]\n    ) -&gt; bool\n\n    def get_image_percentiles(self, image: np.ndarray) -&gt; dict\n</code></pre>"},{"location":"advanced/histogram-normalization/#functions","title":"Functions","text":"<pre><code>def batch_histogram_normalization(\n    input_dir: Union[str, Path],\n    output_dir: Union[str, Path],\n    reference_images: Union[str, Path, List[Union[str, Path]]],\n    percentiles: Tuple[float, float] = (1.0, 99.0),\n    file_pattern: str = '*.png'\n) -&gt; Tuple[int, int]\n\ndef compare_histograms(\n    image1: np.ndarray,\n    image2: np.ndarray,\n    bins: int = 256\n) -&gt; dict\n</code></pre>"},{"location":"advanced/histogram-normalization/#testing","title":"Testing","text":"<p>The module includes comprehensive tests in <code>tests/test_color_normalization.py</code>:</p> <pre><code># Run tests (requires pytest and dependencies installed)\npytest tests/test_color_normalization.py -v\n</code></pre> <p>Test coverage includes: - Normalizer initialization - Image normalization - Batch processing - Histogram comparison - Integration scenarios - Edge cases (uniform images, outliers)</p>"},{"location":"advanced/histogram-normalization/#performance","title":"Performance","text":"<ul> <li>Processing time: ~5-20ms per 512\u00d7512 image (CPU)</li> <li>Memory usage: ~2\u00d7 image size (for percentile computation)</li> <li>Scalability: Linear with number of images</li> </ul>"},{"location":"advanced/histogram-normalization/#choosing-percentile-range","title":"Choosing Percentile Range","text":"Percentile Range Use Case Effect (0.5, 99.5) Very robust Clips more outliers, may lose some detail (1.0, 99.0) Default, recommended Good balance (2.0, 98.0) Aggressive clipping Handles extreme outliers well (5.0, 95.0) Very aggressive May over-normalize"},{"location":"advanced/histogram-normalization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"advanced/histogram-normalization/#issue-normalized-images-look-washed-out","title":"Issue: Normalized images look washed out","text":"<p>Try using a narrower percentile range: <pre><code>normalizer = HistogramPercentileNormalizer(\n    'reference.png',\n    percentiles=(2.0, 98.0)  # More aggressive clipping\n)\n</code></pre></p>"},{"location":"advanced/histogram-normalization/#issue-colors-still-look-different","title":"Issue: Colors still look different","text":"<ul> <li>Ensure reference images are representative of desired appearance</li> <li>Try using multiple reference images for more robust statistics</li> <li>Consider using Reinhard color transfer instead (in <code>color_correction.py</code>)</li> </ul>"},{"location":"advanced/histogram-normalization/#issue-normalization-too-aggressive","title":"Issue: Normalization too aggressive","text":"<p>Use a wider percentile range: <pre><code>normalizer = HistogramPercentileNormalizer(\n    'reference.png',\n    percentiles=(0.5, 99.5)  # Less aggressive\n)\n</code></pre></p>"},{"location":"advanced/histogram-normalization/#comparison-with-reinhard-method","title":"Comparison with Reinhard Method","text":"<p>Both methods are available in the autoslide package:</p> <p>Histogram Percentile (this module): - Simpler, faster - Works in RGB/BGR space - Best for intensity normalization - More robust to outliers</p> <p>Reinhard Color Transfer (<code>color_correction.py</code>): - More sophisticated - Works in LAB color space - Best for color characteristic transfer - Preserves perceptual color relationships</p> <p>Choose based on your specific needs: - Use Histogram Percentile for staining intensity variations - Use Reinhard for general color correction and color space transformations</p>"},{"location":"advanced/histogram-normalization/#references","title":"References","text":"<ol> <li>Histogram equalization and percentile-based methods are standard techniques in image processing</li> <li>Commonly used in medical image analysis for intensity normalization</li> <li>Related to histogram matching and specification techniques</li> </ol>"},{"location":"advanced/histogram-normalization/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements: - Adaptive percentile selection based on image content - Per-tissue-type normalization - Integration with stain separation methods - GPU acceleration for batch processing</p>"},{"location":"advanced/histogram-normalization/#contributing","title":"Contributing","text":"<p>When adding new normalization methods: 1. Add method to <code>HistogramPercentileNormalizer</code> class or create new class 2. Add corresponding tests in <code>tests/test_color_normalization.py</code> 3. Update this documentation 4. Ensure backward compatibility</p>"},{"location":"advanced/histogram-normalization/#license","title":"License","text":"<p>This module is part of the autoslide project and follows the same MIT license.</p>"},{"location":"api/models/","title":"Models API Reference","text":"<p>API documentation for AutoSlide model training and inference modules.</p>"},{"location":"api/models/#model-architecture","title":"Model Architecture","text":""},{"location":"api/models/#mask-r-cnn","title":"Mask R-CNN","text":"<p>The vessel detection model uses Mask R-CNN with ResNet-50 backbone.</p> <pre><code>def get_model(num_classes: int = 2) -&gt; torch.nn.Module:\n    \"\"\"\n    Get Mask R-CNN model for vessel detection.\n\n    Args:\n        num_classes: Number of classes (background + vessel)\n\n    Returns:\n        Mask R-CNN model instance\n    \"\"\"\n</code></pre>"},{"location":"api/models/#training","title":"Training","text":""},{"location":"api/models/#trainpy","title":"train.py","text":"<pre><code>def train_model(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    num_epochs: int = 50,\n    learning_rate: float = 0.005,\n    batch_size: int = 4,\n    output_dir: str = \"artifacts/\"\n) -&gt; dict:\n    \"\"\"\n    Train vessel detection model.\n\n    Args:\n        train_dataset: Training dataset\n        val_dataset: Validation dataset\n        num_epochs: Number of training epochs\n        learning_rate: Initial learning rate\n        batch_size: Training batch size\n        output_dir: Directory to save model checkpoints\n\n    Returns:\n        Dictionary with training metrics\n    \"\"\"\n</code></pre>"},{"location":"api/models/#inference","title":"Inference","text":""},{"location":"api/models/#predictionpy","title":"prediction.py","text":"<pre><code>def load_model(model_path: str, device: str = \"cuda\") -&gt; torch.nn.Module:\n    \"\"\"\n    Load pre-trained model for inference.\n\n    Args:\n        model_path: Path to model weights (.pth)\n        device: Device for inference (\"cuda\" or \"cpu\")\n\n    Returns:\n        Loaded model in eval mode\n    \"\"\"\n\ndef predict_batch(\n    model: torch.nn.Module,\n    images: list,\n    confidence_threshold: float = 0.5\n) -&gt; list:\n    \"\"\"\n    Run inference on batch of images.\n\n    Args:\n        model: Loaded model instance\n        images: List of image tensors\n        confidence_threshold: Minimum detection confidence\n\n    Returns:\n        List of prediction dictionaries\n    \"\"\"\n</code></pre>"},{"location":"api/models/#data-augmentation","title":"Data Augmentation","text":""},{"location":"api/models/#augmentationpy","title":"augmentation.py","text":"<pre><code>def augment_training_data(\n    images: list,\n    masks: list,\n    augmentation_factor: int = 3\n) -&gt; tuple:\n    \"\"\"\n    Apply data augmentation to training samples.\n\n    Args:\n        images: List of training images\n        masks: List of corresponding masks\n        augmentation_factor: Number of augmented versions per sample\n\n    Returns:\n        Tuple of (augmented_images, augmented_masks)\n    \"\"\"\n</code></pre>"},{"location":"api/models/#evaluation","title":"Evaluation","text":""},{"location":"api/models/#evaluatepy","title":"evaluate.py","text":"<pre><code>def evaluate_model(\n    model: torch.nn.Module,\n    test_dataset: Dataset,\n    iou_threshold: float = 0.5\n) -&gt; dict:\n    \"\"\"\n    Evaluate model performance on test set.\n\n    Args:\n        model: Model to evaluate\n        test_dataset: Test dataset\n        iou_threshold: IoU threshold for positive detection\n\n    Returns:\n        Dictionary with evaluation metrics (mAP, precision, recall)\n    \"\"\"\n</code></pre>"},{"location":"api/models/#next-steps","title":"Next Steps","text":"<ul> <li>Pipeline API</li> <li>Utils API</li> <li>Vessel Detection</li> </ul>"},{"location":"api/pipeline/","title":"Pipeline API Reference","text":"<p>API documentation for AutoSlide pipeline modules.</p>"},{"location":"api/pipeline/#core-pipeline-functions","title":"Core Pipeline Functions","text":""},{"location":"api/pipeline/#run_pipelinepy","title":"run_pipeline.py","text":"<p>Main pipeline orchestration module.</p> <pre><code>def run_pipeline(\n    data_dir: str,\n    skip_annotation: bool = False,\n    config: dict = None\n) -&gt; dict:\n    \"\"\"\n    Run the complete AutoSlide pipeline.\n\n    Args:\n        data_dir: Path to data directory\n        skip_annotation: Skip annotation stages if True\n        config: Optional configuration dictionary\n\n    Returns:\n        Dictionary containing pipeline results and metadata\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#annotation-modules","title":"Annotation Modules","text":""},{"location":"api/pipeline/#initial_annotationpy","title":"initial_annotation.py","text":"<pre><code>def initial_annotation(\n    slide_path: str,\n    output_dir: str,\n    threshold_method: str = \"otsu\"\n) -&gt; dict:\n    \"\"\"\n    Perform initial tissue annotation.\n\n    Args:\n        slide_path: Path to .svs slide file\n        output_dir: Directory for output files\n        threshold_method: Thresholding method (\"otsu\", \"adaptive\", \"manual\")\n\n    Returns:\n        Dictionary with annotation results\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#final_annotationpy","title":"final_annotation.py","text":"<pre><code>def final_annotation(\n    slide_path: str,\n    initial_annotation_path: str,\n    labelbox_export: str = None,\n    output_dir: str = None\n) -&gt; dict:\n    \"\"\"\n    Perform final tissue annotation with label integration.\n\n    Args:\n        slide_path: Path to .svs slide file\n        initial_annotation_path: Path to initial annotation results\n        labelbox_export: Optional path to Labelbox NDJSON export\n        output_dir: Directory for output files\n\n    Returns:\n        Dictionary with final annotation results\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#region-suggestion","title":"Region Suggestion","text":""},{"location":"api/pipeline/#suggest_regionspy","title":"suggest_regions.py","text":"<pre><code>def suggest_regions(\n    annotation_path: str,\n    slide_path: str,\n    output_dir: str,\n    region_size: tuple = (512, 512),\n    min_tissue_ratio: float = 0.5\n) -&gt; list:\n    \"\"\"\n    Extract optimal regions for analysis.\n\n    Args:\n        annotation_path: Path to annotation results\n        slide_path: Path to .svs slide file\n        output_dir: Directory for output files\n        region_size: Dimensions of extracted regions\n        min_tissue_ratio: Minimum tissue content ratio\n\n    Returns:\n        List of extracted region metadata\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#model-inference","title":"Model Inference","text":""},{"location":"api/pipeline/#predictionpy","title":"prediction.py","text":"<pre><code>def predict_vessels(\n    image_dir: str,\n    model_path: str,\n    output_dir: str,\n    confidence_threshold: float = 0.5,\n    batch_size: int = 4\n) -&gt; dict:\n    \"\"\"\n    Detect vessels using Mask R-CNN.\n\n    Args:\n        image_dir: Directory containing region images\n        model_path: Path to model weights (.pth)\n        output_dir: Directory for output files\n        confidence_threshold: Minimum detection confidence\n        batch_size: Batch size for inference\n\n    Returns:\n        Dictionary with prediction results\n    \"\"\"\n</code></pre>"},{"location":"api/pipeline/#utility-functions","title":"Utility Functions","text":""},{"location":"api/pipeline/#utilspy","title":"utils.py","text":"<pre><code>def load_slide(slide_path: str, level: int = 0) -&gt; np.ndarray:\n    \"\"\"Load slide image at specified pyramid level.\"\"\"\n\ndef save_mask(mask: np.ndarray, output_path: str) -&gt; None:\n    \"\"\"Save binary mask as PNG.\"\"\"\n\ndef generate_section_hash(\n    slide_id: str,\n    coordinates: tuple,\n    params: dict\n) -&gt; str:\n    \"\"\"Generate unique SHA-256 hash for section tracking.\"\"\"\n\ndef overlay_mask(\n    image: np.ndarray,\n    mask: np.ndarray,\n    alpha: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"Create overlay visualization of mask on image.\"\"\"\n</code></pre>"},{"location":"api/pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Models API</li> <li>Utils API</li> <li>Pipeline Overview</li> </ul>"},{"location":"api/utils/","title":"Utils API Reference","text":"<p>API documentation for AutoSlide utility functions.</p>"},{"location":"api/utils/#image-processing","title":"Image Processing","text":""},{"location":"api/utils/#image-loading","title":"Image Loading","text":"<pre><code>def load_slide(\n    slide_path: str,\n    level: int = 0,\n    region: tuple = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Load whole slide image or region.\n\n    Args:\n        slide_path: Path to .svs file\n        level: Pyramid level (0 = highest resolution)\n        region: Optional (x, y, width, height) tuple\n\n    Returns:\n        Image as numpy array\n    \"\"\"\n</code></pre>"},{"location":"api/utils/#image-preprocessing","title":"Image Preprocessing","text":"<pre><code>def preprocess_image(\n    image: np.ndarray,\n    target_size: tuple = None,\n    normalize: bool = True\n) -&gt; np.ndarray:\n    \"\"\"\n    Preprocess image for model input.\n\n    Args:\n        image: Input image\n        target_size: Optional resize dimensions\n        normalize: Apply normalization if True\n\n    Returns:\n        Preprocessed image\n    \"\"\"\n</code></pre>"},{"location":"api/utils/#mask-operations","title":"Mask Operations","text":""},{"location":"api/utils/#mask-generation","title":"Mask Generation","text":"<pre><code>def create_binary_mask(\n    polygons: list,\n    image_shape: tuple\n) -&gt; np.ndarray:\n    \"\"\"\n    Create binary mask from polygon annotations.\n\n    Args:\n        polygons: List of polygon coordinates\n        image_shape: Output mask dimensions\n\n    Returns:\n        Binary mask as numpy array\n    \"\"\"\n</code></pre>"},{"location":"api/utils/#mask-visualization","title":"Mask Visualization","text":"<pre><code>def overlay_mask(\n    image: np.ndarray,\n    mask: np.ndarray,\n    color: tuple = (255, 0, 0),\n    alpha: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Create overlay visualization.\n\n    Args:\n        image: Base image\n        mask: Binary mask\n        color: Overlay color (RGB)\n        alpha: Transparency (0-1)\n\n    Returns:\n        Overlay image\n    \"\"\"\n</code></pre>"},{"location":"api/utils/#section-tracking","title":"Section Tracking","text":""},{"location":"api/utils/#hash-generation","title":"Hash Generation","text":"<pre><code>def generate_section_hash(\n    slide_id: str,\n    coordinates: tuple,\n    params: dict = None\n) -&gt; str:\n    \"\"\"\n    Generate unique SHA-256 hash for section.\n\n    Args:\n        slide_id: Slide identifier\n        coordinates: (x, y, width, height) tuple\n        params: Optional extraction parameters\n\n    Returns:\n        SHA-256 hash string\n    \"\"\"\n</code></pre>"},{"location":"api/utils/#metadata-management","title":"Metadata Management","text":"<pre><code>def save_section_metadata(\n    sections: list,\n    output_path: str\n) -&gt; None:\n    \"\"\"\n    Save section tracking metadata to JSON.\n\n    Args:\n        sections: List of section dictionaries\n        output_path: Output JSON file path\n    \"\"\"\n\ndef load_section_metadata(\n    metadata_path: str\n) -&gt; list:\n    \"\"\"\n    Load section tracking metadata from JSON.\n\n    Args:\n        metadata_path: Path to metadata JSON\n\n    Returns:\n        List of section dictionaries\n    \"\"\"\n</code></pre>"},{"location":"api/utils/#file-io","title":"File I/O","text":""},{"location":"api/utils/#save-functions","title":"Save Functions","text":"<pre><code>def save_image(\n    image: np.ndarray,\n    output_path: str,\n    quality: int = 95\n) -&gt; None:\n    \"\"\"Save image to file.\"\"\"\n\ndef save_mask(\n    mask: np.ndarray,\n    output_path: str\n) -&gt; None:\n    \"\"\"Save binary mask as PNG.\"\"\"\n\ndef save_results_csv(\n    results: list,\n    output_path: str\n) -&gt; None:\n    \"\"\"Save results to CSV file.\"\"\"\n</code></pre>"},{"location":"api/utils/#color-space-conversions","title":"Color Space Conversions","text":"<pre><code>def rgb_to_hsv(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert RGB image to HSV color space.\"\"\"\n\ndef hsv_to_rgb(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert HSV image to RGB color space.\"\"\"\n</code></pre>"},{"location":"api/utils/#morphological-operations","title":"Morphological Operations","text":"<pre><code>def apply_morphology(\n    mask: np.ndarray,\n    operation: str = \"close\",\n    kernel_size: int = 5\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply morphological operation to mask.\n\n    Args:\n        mask: Binary mask\n        operation: \"open\", \"close\", \"dilate\", or \"erode\"\n        kernel_size: Size of structuring element\n\n    Returns:\n        Processed mask\n    \"\"\"\n</code></pre>"},{"location":"api/utils/#next-steps","title":"Next Steps","text":"<ul> <li>Pipeline API</li> <li>Models API</li> <li>Pipeline Overview</li> </ul>"},{"location":"data/dvc-setup/","title":"DVC Setup","text":"<p>This project uses DVC (Data Version Control) to track large files and model artifacts.</p>"},{"location":"data/dvc-setup/#installation","title":"Installation","text":"<p>Install DVC with your preferred storage backend:</p> <pre><code># Base installation\npip install dvc\n\n# For Google Drive storage\npip install dvc[gdrive]\n\n# For S3 storage\npip install dvc[s3]\n\n# For Azure storage\npip install dvc[azure]\n</code></pre>"},{"location":"data/dvc-setup/#initial-setup","title":"Initial Setup","text":"<p>DVC is already initialized in this repository. To configure remote storage:</p>"},{"location":"data/dvc-setup/#google-drive","title":"Google Drive","text":"<pre><code>dvc remote add -d myremote gdrive://folder_id\n</code></pre>"},{"location":"data/dvc-setup/#amazon-s3","title":"Amazon S3","text":"<pre><code>dvc remote add -d myremote s3://bucket/path\n</code></pre>"},{"location":"data/dvc-setup/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>dvc remote add -d myremote azure://container/path\n</code></pre>"},{"location":"data/dvc-setup/#working-with-dvc","title":"Working with DVC","text":""},{"location":"data/dvc-setup/#adding-files-to-dvc","title":"Adding Files to DVC","text":"<p>Track large files or directories:</p> <pre><code># Add a large dataset\ndvc add data/large_dataset.svs\n\n# Add model artifacts\ndvc add artifacts/mask_rcnn_model.pth\n\n# Add entire directories\ndvc add data/slides/\n</code></pre> <p>This creates <code>.dvc</code> files that you commit to git instead of the large files.</p>"},{"location":"data/dvc-setup/#committing-changes","title":"Committing Changes","text":"<pre><code># Stage the .dvc file\ngit add data/large_dataset.svs.dvc\n\n# Commit to git\ngit commit -m \"Add large dataset\"\n\n# Push data to remote storage\ndvc push\n</code></pre>"},{"location":"data/dvc-setup/#pulling-data","title":"Pulling Data","text":"<p>Retrieve data tracked by DVC:</p> <pre><code># Pull all tracked data\ndvc pull\n\n# Pull specific file\ndvc pull data/large_dataset.svs.dvc\n</code></pre>"},{"location":"data/dvc-setup/#versioning-models","title":"Versioning Models","text":"<p>When you train a new model version:</p> <pre><code># Add the new model\ndvc add artifacts/mask_rcnn_model.pth\n\n# Commit the .dvc file\ngit add artifacts/mask_rcnn_model.pth.dvc\ngit commit -m \"Update model with improved accuracy\"\n\n# Push to remote storage\ndvc push\n</code></pre>"},{"location":"data/dvc-setup/#switching-between-versions","title":"Switching Between Versions","text":"<pre><code># Checkout a specific commit\ngit checkout &lt;commit-hash&gt;\n\n# Get the corresponding data\ndvc pull\n</code></pre>"},{"location":"data/dvc-setup/#pipeline-integration","title":"Pipeline Integration","text":"<p>DVC can track pipeline outputs:</p> <pre><code># Track pipeline outputs\ndvc add output/predictions/\ndvc add output/fibrosis/\n\n# Commit and push\ngit add output/predictions.dvc output/fibrosis.dvc\ngit commit -m \"Add pipeline outputs\"\ndvc push\n</code></pre>"},{"location":"data/dvc-setup/#best-practices","title":"Best Practices","text":"<ol> <li>Always use DVC for large files - Don't commit large files directly to git</li> <li>Commit .dvc files to git - These small metadata files track your data</li> <li>Push after adding - Run <code>dvc push</code> to make data available to collaborators</li> <li>Pull after checkout - Run <code>dvc pull</code> after switching branches/commits</li> <li>Use .gitignore - Ensure tracked files are in .gitignore</li> </ol>"},{"location":"data/dvc-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data/dvc-setup/#authentication-issues","title":"Authentication Issues","text":"<p>For Google Drive:</p> <pre><code>dvc remote modify myremote gdrive_use_service_account true\ndvc remote modify myremote gdrive_service_account_json_file_path path/to/credentials.json\n</code></pre> <p>For S3:</p> <pre><code>dvc remote modify myremote access_key_id &lt;key&gt;\ndvc remote modify myremote secret_access_key &lt;secret&gt;\n</code></pre>"},{"location":"data/dvc-setup/#cache-issues","title":"Cache Issues","text":"<p>Clear DVC cache if needed:</p> <pre><code>dvc cache dir  # Show cache location\nrm -rf .dvc/cache  # Clear cache (use with caution)\n</code></pre>"},{"location":"data/dvc-setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>DVC Documentation</li> <li>DVC with Google Drive</li> <li>DVC with S3</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>AutoSlide uses a simple JSON configuration file to manage pipeline settings.</p>"},{"location":"getting-started/configuration/#configuration-file","title":"Configuration File","text":"<p>The main configuration file is located at <code>src/config.json</code>:</p> <pre><code>{\n    \"data_dir\": \"/path/to/your/data\"\n}\n</code></pre>"},{"location":"getting-started/configuration/#data-directory-structure","title":"Data Directory Structure","text":"<p>Your data directory should be organized as follows:</p> <pre><code>data_dir/\n\u251c\u2500\u2500 slides/              # Raw .svs slide files\n\u251c\u2500\u2500 annotations/         # Annotation files (NDJSON, CSV)\n\u251c\u2500\u2500 models/             # Pre-trained model files (.pth)\n\u2514\u2500\u2500 output/             # Pipeline output directory\n    \u251c\u2500\u2500 initial_annotation/\n    \u251c\u2500\u2500 final_annotation/\n    \u251c\u2500\u2500 regions/\n    \u251c\u2500\u2500 predictions/\n    \u2514\u2500\u2500 fibrosis/\n</code></pre>"},{"location":"getting-started/configuration/#pipeline-parameters","title":"Pipeline Parameters","text":""},{"location":"getting-started/configuration/#annotation-parameters","title":"Annotation Parameters","text":"<p>Adjust thresholding and morphological operations in the annotation scripts:</p> <ul> <li>Threshold values - Control tissue detection sensitivity</li> <li>Morphological kernel sizes - Affect region smoothing and cleanup</li> <li>Minimum region size - Filter out small artifacts</li> </ul>"},{"location":"getting-started/configuration/#region-suggestion-parameters","title":"Region Suggestion Parameters","text":"<p>Configure region extraction in <code>suggest_regions.py</code>:</p> <ul> <li>Region size - Dimensions of extracted sections</li> <li>Overlap - Amount of overlap between adjacent regions</li> <li>Selection criteria - Tissue density, vessel presence, etc.</li> </ul>"},{"location":"getting-started/configuration/#vessel-detection-parameters","title":"Vessel Detection Parameters","text":"<p>Model inference settings in <code>model/prediction.py</code>:</p> <ul> <li>Confidence threshold - Minimum detection confidence</li> <li>NMS threshold - Non-maximum suppression threshold</li> <li>Batch size - Number of images processed simultaneously</li> </ul>"},{"location":"getting-started/configuration/#fibrosis-quantification-parameters","title":"Fibrosis Quantification Parameters","text":"<p>HSV color analysis settings:</p> <pre><code>python src/fibrosis_calculation/calc_fibrosis.py \\\n    --hue-value 0.6785 \\\n    --hue-width 0.4 \\\n    --verbose\n</code></pre> <ul> <li>hue-value - Target hue for fibrotic tissue</li> <li>hue-width - Tolerance around target hue</li> <li>verbose - Enable detailed logging</li> </ul>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Set environment variables for additional configuration:</p> <pre><code>export AUTOSLIDE_DATA_DIR=/path/to/data\nexport AUTOSLIDE_MODEL_PATH=/path/to/model.pth\nexport CUDA_VISIBLE_DEVICES=0  # GPU selection\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<p>For advanced users, modify pipeline behavior directly in the source code:</p> <ul> <li><code>src/pipeline/utils.py</code> - Core utility functions</li> <li><code>src/pipeline/annotation/</code> - Annotation algorithms</li> <li><code>src/pipeline/model/</code> - Model training and inference</li> </ul>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Pipeline Overview</li> <li>API Reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>AutoSlide requires Python 3.8 or higher and leverages powerful Python libraries for image processing and deep learning.</p>"},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>slideio - For efficient slide image handling (.svs format support)</li> <li>PyTorch - For deep learning model training and inference</li> <li>torchvision - For computer vision models and transforms</li> <li>scikit-learn - For clustering and dimensionality reduction</li> <li>OpenCV - For advanced image processing</li> <li>matplotlib/pandas - For visualization and data handling</li> <li>PIL/Pillow - For image manipulation</li> <li>tqdm - For progress tracking</li> <li>numpy/scipy - For numerical computations</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/pulakatlab/autoslide.git\ncd autoslide\n</code></pre>"},{"location":"getting-started/installation/#2-create-a-virtual-environment-recommended","title":"2. Create a Virtual Environment (Recommended)","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>For development dependencies:</p> <pre><code>pip install -r requirements-dev.txt\n</code></pre>"},{"location":"getting-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code>python -c \"import autoslide; print('AutoSlide installed successfully!')\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dvc-setup","title":"Optional: DVC Setup","text":"<p>If you need to work with versioned data and models, install DVC:</p> <pre><code>pip install dvc\n\n# For Google Drive storage\npip install dvc[gdrive]\n\n# For S3 storage\npip install dvc[s3]\n</code></pre> <p>See the DVC Setup Guide for more details.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#slideio-installation-issues","title":"SlideIO Installation Issues","text":"<p>If you encounter issues installing slideio, ensure you have the necessary system dependencies:</p> <p>Ubuntu/Debian: <pre><code>sudo apt-get update\nsudo apt-get install libopenslide-dev\n</code></pre></p> <p>macOS: <pre><code>brew install openslide\n</code></pre></p>"},{"location":"getting-started/installation/#pytorch-installation","title":"PyTorch Installation","text":"<p>For GPU support, install PyTorch with CUDA:</p> <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n</code></pre> <p>Visit PyTorch's website for platform-specific instructions.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Configuration</li> <li>Pipeline Overview</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with AutoSlide in minutes.</p>"},{"location":"getting-started/quickstart/#running-the-complete-pipeline","title":"Running the Complete Pipeline","text":"<p>Execute the entire pipeline with a single command:</p> <pre><code>python src/pipeline/run_pipeline.py\n</code></pre> <p>This will run all pipeline stages in sequence:</p> <ol> <li>Initial annotation</li> <li>Final annotation</li> <li>Region suggestion</li> <li>Vessel detection</li> <li>Fibrosis quantification</li> </ol>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>Before running the pipeline, set up your data directory in <code>src/config.json</code>:</p> <pre><code>{\n    \"data_dir\": \"/path/to/your/data\"\n}\n</code></pre>"},{"location":"getting-started/quickstart/#running-individual-pipeline-steps","title":"Running Individual Pipeline Steps","text":"<p>You can execute specific stages independently:</p>"},{"location":"getting-started/quickstart/#initial-annotation","title":"Initial Annotation","text":"<pre><code>python src/pipeline/annotation/initial_annotation.py\n</code></pre>"},{"location":"getting-started/quickstart/#final-annotation","title":"Final Annotation","text":"<pre><code>python src/pipeline/annotation/final_annotation.py\n</code></pre>"},{"location":"getting-started/quickstart/#region-suggestion","title":"Region Suggestion","text":"<pre><code>python src/pipeline/suggest_regions.py\n</code></pre>"},{"location":"getting-started/quickstart/#vessel-detection","title":"Vessel Detection","text":"<pre><code>python src/pipeline/model/prediction.py\n</code></pre>"},{"location":"getting-started/quickstart/#fibrosis-quantification","title":"Fibrosis Quantification","text":"<pre><code>python src/fibrosis_calculation/calc_fibrosis.py\n</code></pre>"},{"location":"getting-started/quickstart/#command-line-options","title":"Command Line Options","text":""},{"location":"getting-started/quickstart/#skip-annotation-steps","title":"Skip Annotation Steps","text":"<p>If you already have annotations:</p> <pre><code>python src/pipeline/run_pipeline.py --skip_annotation\n</code></pre>"},{"location":"getting-started/quickstart/#fibrosis-quantification-with-custom-parameters","title":"Fibrosis Quantification with Custom Parameters","text":"<pre><code>python src/fibrosis_calculation/calc_fibrosis.py --hue-value 0.6785 --hue-width 0.4 --verbose\n</code></pre>"},{"location":"getting-started/quickstart/#expected-output","title":"Expected Output","text":"<p>AutoSlide generates comprehensive outputs including:</p> <ul> <li>Annotated slide visualizations with tissue boundaries and labels</li> <li>Region selection maps showing extracted analysis areas</li> <li>Vessel detection results with identified structures highlighted</li> <li>Fibrosis quantification reports with HSV-based percentage measurements and visualizations</li> <li>Section tracking data with unique identifiers for reproducibility</li> <li>Quality control reports from the mask validation GUI</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration Options</li> <li>Explore the Pipeline Overview</li> <li>Use the Mask Validation GUI for quality control</li> </ul>"},{"location":"pipeline/fibrosis-quantification/","title":"Fibrosis Quantification","text":"<p>The fibrosis quantification stage measures fibrotic tissue content using HSV color space analysis.</p>"},{"location":"pipeline/fibrosis-quantification/#overview","title":"Overview","text":"<p>This stage analyzes tissue sections to quantify fibrosis based on color characteristics, providing objective percentage measurements of fibrotic content.</p>"},{"location":"pipeline/fibrosis-quantification/#how-it-works","title":"How It Works","text":"<ol> <li>Load Region Images - Import extracted tissue sections</li> <li>Convert to HSV - Transform from RGB to HSV color space</li> <li>Define Fibrosis Range - Set hue parameters for fibrotic tissue</li> <li>Create Mask - Identify pixels matching fibrosis criteria</li> <li>Calculate Percentage - Compute fibrotic area relative to total tissue</li> <li>Generate Visualizations - Create overlay images showing fibrotic regions</li> </ol>"},{"location":"pipeline/fibrosis-quantification/#usage","title":"Usage","text":""},{"location":"pipeline/fibrosis-quantification/#basic-usage","title":"Basic Usage","text":"<pre><code>python src/fibrosis_calculation/calc_fibrosis.py\n</code></pre>"},{"location":"pipeline/fibrosis-quantification/#with-custom-parameters","title":"With Custom Parameters","text":"<pre><code>python src/fibrosis_calculation/calc_fibrosis.py \\\n    --hue-value 0.6785 \\\n    --hue-width 0.4 \\\n    --verbose\n</code></pre>"},{"location":"pipeline/fibrosis-quantification/#parameters","title":"Parameters","text":""},{"location":"pipeline/fibrosis-quantification/#hue-value","title":"Hue Value","text":"<p>The target hue for fibrotic tissue (0.0 - 1.0):</p> <ul> <li>Default: <code>0.6785</code> (blue-purple range typical of fibrosis staining)</li> <li>Adjust based on your staining protocol</li> </ul>"},{"location":"pipeline/fibrosis-quantification/#hue-width","title":"Hue Width","text":"<p>Tolerance around the target hue (0.0 - 1.0):</p> <ul> <li>Default: <code>0.4</code></li> <li>Larger values capture more color variation</li> <li>Smaller values are more selective</li> </ul>"},{"location":"pipeline/fibrosis-quantification/#saturation-and-value-thresholds","title":"Saturation and Value Thresholds","text":"<p>Additional filters to refine detection:</p> <ul> <li>Minimum saturation to exclude pale regions</li> <li>Minimum value to exclude dark artifacts</li> </ul>"},{"location":"pipeline/fibrosis-quantification/#output","title":"Output","text":"<ul> <li>Fibrosis percentage for each region</li> <li>Visualization overlays highlighting fibrotic areas</li> <li>CSV file with quantification results</li> <li>Summary statistics</li> </ul>"},{"location":"pipeline/fibrosis-quantification/#validation","title":"Validation","text":"<p>Results can be validated using:</p> <ol> <li>Manual review of overlay images</li> <li>Comparison with pathologist assessments</li> <li>Statistical analysis across sample cohorts</li> </ol>"},{"location":"pipeline/fibrosis-quantification/#tips-for-accurate-quantification","title":"Tips for Accurate Quantification","text":"<ul> <li>Calibrate parameters - Adjust hue values for your specific staining</li> <li>Quality control - Use the mask validation GUI to review results</li> <li>Consistent staining - Ensure uniform staining across samples</li> <li>Exclude artifacts - Remove regions with staining artifacts before quantification</li> </ul>"},{"location":"pipeline/fibrosis-quantification/#next-steps","title":"Next Steps","text":"<ul> <li>Mask Validation GUI - Review quantification results</li> <li>Color Correction - Normalize staining variations</li> </ul>"},{"location":"pipeline/final-annotation/","title":"Final Annotation","text":"<p>The final annotation stage performs precise tissue labeling and generates high-quality masks for detailed analysis.</p>"},{"location":"pipeline/final-annotation/#overview","title":"Overview","text":"<p>This stage refines the initial annotations, integrates manual labels from annotation tools like Labelbox, and produces final segmentation masks.</p>"},{"location":"pipeline/final-annotation/#how-it-works","title":"How It Works","text":"<ol> <li>Load Initial Annotations - Import results from initial annotation stage</li> <li>Import Manual Labels - Load annotations from Labelbox or other tools</li> <li>Merge Annotations - Combine automated and manual annotations</li> <li>Refine Segmentation - Apply advanced segmentation algorithms</li> <li>Generate Masks - Create binary masks for each tissue class</li> <li>Quality Control - Validate mask quality</li> </ol>"},{"location":"pipeline/final-annotation/#usage","title":"Usage","text":"<pre><code>python src/pipeline/annotation/final_annotation.py\n</code></pre>"},{"location":"pipeline/final-annotation/#integration-with-labelbox","title":"Integration with Labelbox","text":"<p>AutoSlide can import annotations from Labelbox NDJSON exports:</p> <ol> <li>Export annotations from Labelbox in NDJSON format</li> <li>Place exports in the annotations directory</li> <li>Run final annotation to merge with automated results</li> </ol>"},{"location":"pipeline/final-annotation/#output","title":"Output","text":"<ul> <li>Multi-class tissue segmentation masks</li> <li>Labeled region images</li> <li>Annotation metadata in JSON format</li> <li>Quality metrics</li> </ul>"},{"location":"pipeline/final-annotation/#next-steps","title":"Next Steps","text":"<ul> <li>Region Suggestion - Extract analysis regions</li> <li>Mask Validation GUI - Quality control</li> </ul>"},{"location":"pipeline/initial-annotation/","title":"Initial Annotation","text":"<p>The initial annotation stage performs automated tissue detection and region identification in whole slide images.</p>"},{"location":"pipeline/initial-annotation/#overview","title":"Overview","text":"<p>This stage uses adaptive thresholding and morphological operations to identify tissue regions in .svs slide files, providing the foundation for downstream analysis.</p>"},{"location":"pipeline/initial-annotation/#how-it-works","title":"How It Works","text":"<ol> <li>Slide Loading - Load whole slide image at appropriate resolution</li> <li>Preprocessing - Convert to grayscale and apply noise reduction</li> <li>Thresholding - Adaptive thresholding to separate tissue from background</li> <li>Morphological Operations - Clean up detected regions using opening/closing</li> <li>Contour Detection - Extract region boundaries</li> <li>Visualization - Generate annotated images showing detected regions</li> </ol>"},{"location":"pipeline/initial-annotation/#usage","title":"Usage","text":"<pre><code>python src/pipeline/annotation/initial_annotation.py\n</code></pre>"},{"location":"pipeline/initial-annotation/#parameters","title":"Parameters","text":"<p>Key parameters that can be adjusted in the source code:</p> <ul> <li>Threshold method - Otsu, adaptive, or manual threshold</li> <li>Kernel size - Size of morphological operation kernels</li> <li>Minimum area - Minimum region size to keep</li> <li>Resolution level - Slide pyramid level for processing</li> </ul>"},{"location":"pipeline/initial-annotation/#output","title":"Output","text":"<ul> <li>Annotated slide images with detected regions highlighted</li> <li>Binary masks of tissue regions</li> <li>Region metadata (coordinates, areas, etc.)</li> </ul>"},{"location":"pipeline/initial-annotation/#next-steps","title":"Next Steps","text":"<ul> <li>Final Annotation - Refine tissue segmentation</li> <li>Region Suggestion - Extract analysis regions</li> </ul>"},{"location":"pipeline/overview/","title":"Pipeline Overview","text":"<p>AutoSlide's end-to-end workflow transforms raw histological slides into actionable insights through five main stages.</p>"},{"location":"pipeline/overview/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"pipeline/overview/#1-initial-annotation","title":"1. Initial Annotation","text":"<p>Intelligent thresholding and region identification to detect tissue areas in whole slide images.</p> <p>Key Features:</p> <ul> <li>Automated tissue detection using adaptive thresholding</li> <li>Morphological operations for noise reduction</li> <li>Region boundary extraction</li> <li>Preliminary tissue classification</li> </ul> <p>Learn more \u2192</p>"},{"location":"pipeline/overview/#2-final-annotation","title":"2. Final Annotation","text":"<p>Precise tissue labeling and mask generation for detailed analysis.</p> <p>Key Features:</p> <ul> <li>Refined tissue segmentation</li> <li>Multi-class tissue labeling</li> <li>High-quality binary mask generation</li> <li>Integration with manual annotations from Labelbox</li> </ul> <p>Learn more \u2192</p>"},{"location":"pipeline/overview/#3-region-suggestion","title":"3. Region Suggestion","text":"<p>Strategic selection of analysis-ready sections from annotated slides.</p> <p>Key Features:</p> <ul> <li>Context-aware region extraction</li> <li>Tissue density analysis</li> <li>Optimal section selection</li> <li>SHA-256 based unique section tracking</li> </ul> <p>Learn more \u2192</p>"},{"location":"pipeline/overview/#4-vessel-detection","title":"4. Vessel Detection","text":"<p>Deep learning-based identification of vascular structures using pre-trained Mask R-CNN.</p> <p>Key Features:</p> <ul> <li>Pre-trained Mask R-CNN model</li> <li>Instance segmentation of blood vessels</li> <li>Confidence-based filtering</li> <li>Visualization of detection results</li> </ul> <p>Learn more \u2192</p>"},{"location":"pipeline/overview/#5-fibrosis-quantification","title":"5. Fibrosis Quantification","text":"<p>Automated measurement of fibrotic tissue using HSV color analysis.</p> <p>Key Features:</p> <ul> <li>HSV color space analysis</li> <li>Percentage-based quantification</li> <li>Customizable hue parameters</li> <li>Visual overlay generation</li> </ul> <p>Learn more \u2192</p>"},{"location":"pipeline/overview/#data-flow","title":"Data Flow","text":"<pre><code>graph LR\n    A[Raw .svs Slides] --&gt; B[Initial Annotation]\n    B --&gt; C[Final Annotation]\n    C --&gt; D[Region Suggestion]\n    D --&gt; E[Vessel Detection]\n    D --&gt; F[Fibrosis Quantification]\n    E --&gt; G[Analysis Results]\n    F --&gt; G\n</code></pre>"},{"location":"pipeline/overview/#running-the-pipeline","title":"Running the Pipeline","text":""},{"location":"pipeline/overview/#complete-pipeline","title":"Complete Pipeline","text":"<pre><code>python src/pipeline/run_pipeline.py\n</code></pre>"},{"location":"pipeline/overview/#skip-annotation","title":"Skip Annotation","text":"<pre><code>python src/pipeline/run_pipeline.py --skip_annotation\n</code></pre>"},{"location":"pipeline/overview/#individual-stages","title":"Individual Stages","text":"<p>Run specific stages independently for debugging or custom workflows:</p> <pre><code># Initial annotation only\npython src/pipeline/annotation/initial_annotation.py\n\n# Final annotation only\npython src/pipeline/annotation/final_annotation.py\n\n# Region suggestion\npython src/pipeline/suggest_regions.py\n\n# Vessel detection\npython src/pipeline/model/prediction.py\n\n# Fibrosis quantification\npython src/fibrosis_calculation/calc_fibrosis.py\n</code></pre>"},{"location":"pipeline/overview/#output-structure","title":"Output Structure","text":"<pre><code>output/\n\u251c\u2500\u2500 initial_annotation/\n\u2502   \u251c\u2500\u2500 slide_001_annotated.png\n\u2502   \u2514\u2500\u2500 slide_001_masks.png\n\u251c\u2500\u2500 final_annotation/\n\u2502   \u251c\u2500\u2500 slide_001_final.png\n\u2502   \u2514\u2500\u2500 slide_001_labels.json\n\u251c\u2500\u2500 regions/\n\u2502   \u251c\u2500\u2500 slide_001_region_001.png\n\u2502   \u251c\u2500\u2500 slide_001_region_002.png\n\u2502   \u2514\u2500\u2500 region_tracking.json\n\u251c\u2500\u2500 predictions/\n\u2502   \u251c\u2500\u2500 slide_001_region_001_pred.png\n\u2502   \u2514\u2500\u2500 predictions.json\n\u2514\u2500\u2500 fibrosis/\n    \u251c\u2500\u2500 slide_001_fibrosis.png\n    \u2514\u2500\u2500 fibrosis_results.csv\n</code></pre>"},{"location":"pipeline/overview/#quality-control","title":"Quality Control","text":"<p>Use the Mask Validation GUI to review and validate pipeline outputs before downstream analysis.</p>"},{"location":"pipeline/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Initial Annotation Details</li> <li>Final Annotation Details</li> <li>Region Suggestion Details</li> <li>Vessel Detection Details</li> <li>Fibrosis Quantification Details</li> </ul>"},{"location":"pipeline/region-suggestion/","title":"Region Suggestion","text":"<p>The region suggestion stage strategically selects analysis-ready sections from annotated slides.</p>"},{"location":"pipeline/region-suggestion/#overview","title":"Overview","text":"<p>This stage extracts optimal regions for vessel detection and fibrosis quantification based on tissue properties and analysis requirements.</p>"},{"location":"pipeline/region-suggestion/#how-it-works","title":"How It Works","text":"<ol> <li>Load Annotations - Import final annotation results</li> <li>Analyze Tissue Properties - Evaluate tissue density, quality, and characteristics</li> <li>Score Regions - Rank potential regions based on analysis criteria</li> <li>Extract Sections - Cut out selected regions at appropriate resolution</li> <li>Generate Tracking IDs - Create SHA-256 hashes for reproducibility</li> <li>Save Metadata - Store region coordinates and properties</li> </ol>"},{"location":"pipeline/region-suggestion/#usage","title":"Usage","text":"<pre><code>python src/pipeline/suggest_regions.py\n</code></pre>"},{"location":"pipeline/region-suggestion/#selection-criteria","title":"Selection Criteria","text":"<p>Regions are selected based on:</p> <ul> <li>Tissue density - Sufficient tissue content</li> <li>Quality metrics - Minimal artifacts and staining issues</li> <li>Size requirements - Appropriate dimensions for analysis</li> <li>Vessel presence - Likelihood of containing vessels (for vessel detection)</li> <li>Fibrotic characteristics - Suitable for fibrosis quantification</li> </ul>"},{"location":"pipeline/region-suggestion/#unique-section-tracking","title":"Unique Section Tracking","text":"<p>Each extracted region receives a unique SHA-256 hash based on:</p> <ul> <li>Source slide identifier</li> <li>Region coordinates</li> <li>Extraction parameters</li> </ul> <p>This ensures reproducibility and prevents duplicate processing.</p>"},{"location":"pipeline/region-suggestion/#output","title":"Output","text":"<ul> <li>Extracted region images (PNG format)</li> <li>Region tracking JSON with coordinates and metadata</li> <li>Visualization showing selected regions on original slide</li> </ul>"},{"location":"pipeline/region-suggestion/#next-steps","title":"Next Steps","text":"<ul> <li>Vessel Detection - Detect vessels in extracted regions</li> <li>Fibrosis Quantification - Measure fibrosis in regions</li> </ul>"},{"location":"pipeline/vessel-detection/","title":"Vessel Detection","text":"<p>The vessel detection stage uses deep learning to identify and segment blood vessels in tissue sections.</p>"},{"location":"pipeline/vessel-detection/#overview","title":"Overview","text":"<p>This stage applies a pre-trained Mask R-CNN model to detect vascular structures with instance segmentation, providing precise vessel boundaries and locations.</p>"},{"location":"pipeline/vessel-detection/#how-it-works","title":"How It Works","text":"<ol> <li>Load Pre-trained Model - Import Mask R-CNN weights</li> <li>Prepare Images - Preprocess extracted regions for inference</li> <li>Run Inference - Detect vessels using the model</li> <li>Post-processing - Apply confidence thresholding and NMS</li> <li>Generate Visualizations - Create overlay images showing detections</li> <li>Save Results - Export predictions in JSON format</li> </ol>"},{"location":"pipeline/vessel-detection/#usage","title":"Usage","text":"<pre><code>python src/pipeline/model/prediction.py\n</code></pre>"},{"location":"pipeline/vessel-detection/#model-architecture","title":"Model Architecture","text":"<ul> <li>Backbone: ResNet-50 with Feature Pyramid Network (FPN)</li> <li>Detection Head: Mask R-CNN for instance segmentation</li> <li>Training Data: Histological slides with manually annotated vessels</li> </ul>"},{"location":"pipeline/vessel-detection/#parameters","title":"Parameters","text":"<p>Adjust detection parameters:</p> <pre><code># Confidence threshold\nconfidence_threshold = 0.5\n\n# Non-maximum suppression threshold\nnms_threshold = 0.3\n\n# Batch size for inference\nbatch_size = 4\n</code></pre>"},{"location":"pipeline/vessel-detection/#output","title":"Output","text":"<ul> <li>Predicted vessel masks for each region</li> <li>Bounding boxes and confidence scores</li> <li>Visualization overlays</li> <li>JSON file with detection metadata</li> </ul>"},{"location":"pipeline/vessel-detection/#performance","title":"Performance","text":"<p>The pre-trained model achieves:</p> <ul> <li>High precision on vessel detection</li> <li>Robust performance across different tissue types</li> <li>Fast inference suitable for batch processing</li> </ul>"},{"location":"pipeline/vessel-detection/#advanced-usage","title":"Advanced Usage","text":"<p>For custom predictions on arbitrary directories, see Arbitrary Directory Prediction.</p>"},{"location":"pipeline/vessel-detection/#next-steps","title":"Next Steps","text":"<ul> <li>Fibrosis Quantification - Measure fibrosis</li> <li>Mask Validation GUI - Review predictions</li> </ul>"},{"location":"tools/mask-validation-gui/","title":"Mask Validation GUI","text":"<p>GUI tool for validating masks generated by the AutoSlide pipeline.</p>"},{"location":"tools/mask-validation-gui/#features","title":"Features","text":"<ul> <li>Three-panel display: Original image, mask, and overlay side-by-side</li> <li>Visual indicators: Red diagonal lines mark dropped masks/images</li> <li>Mask dropped: Lines on mask and overlay panels</li> <li>Image set dropped: Lines on all three panels</li> <li>Keyboard navigation: Up/Down arrows to move through images</li> <li>Quick validation: Left arrow to drop mask, Right arrow to drop entire image set</li> <li>Autosave: Automatically saves validation results every 30 seconds</li> <li>Persistent state: Validation results saved to JSON file</li> <li>Auto-overlay generation: Creates overlay with 0.3 alpha if not present</li> </ul>"},{"location":"tools/mask-validation-gui/#usage","title":"Usage","text":""},{"location":"tools/mask-validation-gui/#command-line","title":"Command Line","text":"<pre><code># Point to a directory containing images\npython autoslide/src/utils/mask_validation_gui.py /path/to/suggested_regions/SVS_NAME/SCENE_NAME\n\n# Or run without arguments to get a directory picker\npython autoslide/src/utils/mask_validation_gui.py\n</code></pre>"},{"location":"tools/mask-validation-gui/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<ul> <li>Up Arrow: Previous image</li> <li>Down Arrow: Next image</li> <li>Left Arrow: Drop/restore mask for current image</li> <li>Right Arrow: Drop/restore entire image set</li> <li>S: Save validation results</li> <li>Q: Quit (prompts to save if changes exist)</li> </ul>"},{"location":"tools/mask-validation-gui/#directory-structure","title":"Directory Structure","text":"<p>The GUI expects the standard AutoSlide pipeline structure:</p> <pre><code>directory/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image_001.png\n\u2502   \u251c\u2500\u2500 image_002.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 masks/\n\u2502   \u251c\u2500\u2500 image_001_mask.png\n\u2502   \u251c\u2500\u2500 image_002_mask.png\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 overlays/ (optional)\n    \u251c\u2500\u2500 image_001_overlay.png\n    \u251c\u2500\u2500 image_002_overlay.png\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"tools/mask-validation-gui/#output","title":"Output","text":"<p>Validation results are saved to <code>validation_results.json</code> in the selected directory:</p> <pre><code>{\n  \"dropped_masks\": [\n    \"/path/to/image_001.png\",\n    \"/path/to/image_003.png\"\n  ],\n  \"dropped_images\": [\n    \"/path/to/image_005.png\"\n  ],\n  \"total_images\": 50,\n  \"num_dropped_masks\": 2,\n  \"num_dropped_images\": 1\n}\n</code></pre>"},{"location":"tools/mask-validation-gui/#requirements","title":"Requirements","text":"<ul> <li>Python 3.7+</li> <li>tkinter (usually included with Python)</li> <li>Pillow (PIL)</li> <li>numpy</li> </ul> <p>All dependencies are included in the project's <code>requirements.txt</code>.</p>"}]}